{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2. Linear regression with one variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loss: 32.072735\n",
      "step (1500) => loss: 4.4833879471\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 0.01\n",
    "NUM_STEPS = 1500\n",
    "\n",
    "data = np.loadtxt(\"./ex1/ex1data1.txt\", delimiter=',')\n",
    "\n",
    "x_data = np.reshape(np.array(data[:, 0]), [-1, 1]) # (97, 1)\n",
    "y_data = np.reshape(np.array(data[:, 1]), [-1, 1]) # (97, 1)\n",
    "\n",
    "X = tf.placeholder(dtype=tf.float32, shape=[None, 1])\n",
    "Y = tf.placeholder(dtype=tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.zeros([1, 1]))\n",
    "B = tf.Variable(tf.zeros([1, 1]))\n",
    "\n",
    "pred = tf.matmul(X, W) + B\n",
    "\n",
    "#loss = 1/2 * tf.reduce_mean(tf.square(pred - Y))\n",
    "loss = 1/2 * tf.losses.mean_squared_error(labels=Y, predictions=pred)\n",
    "optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "print(\"initial loss: \"+str(sess.run(loss, feed_dict = {X: x_data, Y: y_data})))\n",
    "index = 0\n",
    "for step in range(NUM_STEPS):\n",
    "    sess.run(train, feed_dict = {X: x_data, Y: y_data})\n",
    "    index = index + 1\n",
    "\n",
    "loss = sess.run(loss, feed_dict = {X: x_data, Y: y_data})\n",
    "print(\"step (\"+str(index)+\") => loss: \"+str(format(loss, '.10f')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Linear Regression with Multiple Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loss: 0.027238019\n",
      "model: [0.95241016] * x1 + [-0.06594656] * x2 + [[1.4551915e-09]]\n",
      "loss: 0.0072740475\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 1\n",
    "NUM_STEPS = 1000\n",
    "\n",
    "data = np.loadtxt(\"./ex1/ex1data2.txt\", delimiter=',')\n",
    "\n",
    "x = np.array(data[:, :-1]) # (47, 2) matrix\n",
    "y = np.array(np.reshape(data[:, -1], [-1,1])) # 47-dimension vector => (47, 1) matrix\n",
    "\n",
    "norm_x = (x - x.mean(axis=0)) / (x.max(axis=0) - x.min(axis=0))\n",
    "norm_y = (y - y.mean(axis=0)) / (y.max(axis=0) - y.min(axis=0))\n",
    "\n",
    "n = np.shape(x)[1] # number of features = 2\n",
    "m = np.shape(x)[0] # number of training examples = 47\n",
    "\n",
    "norm_X = tf.placeholder(dtype=tf.float32, shape=[None, 2])\n",
    "norm_Y = tf.placeholder(dtype=tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.zeros([2, 1])) # (2, 1) matrix\n",
    "# B = tf.Variable(tf.zeros([m, 1])) # (47, 1) matrix\n",
    "B = tf.Variable(tf.zeros([1, 1])) # (1, 1) matrix\n",
    "\n",
    "pred = tf.matmul(norm_X, W) + B # (47, 2) * (2, 1) + (47, 1) = (47, 1)\n",
    "\n",
    "# loss = 1/2 * tf.reduce_mean(tf.square(pred - norm_Y))\n",
    "loss = 1/2 *tf.losses.mean_squared_error(labels=norm_Y, predictions=pred)\n",
    "optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "print(\"initial loss: \"+str(sess.run(loss, feed_dict = {norm_X: norm_x, norm_Y: norm_y})))\n",
    "\n",
    "for step in range(NUM_STEPS):\n",
    "    sess.run(train, feed_dict={norm_X: norm_x, norm_Y: norm_y})\n",
    "\n",
    "weight, bias, final_loss = sess.run([W, B, loss], feed_dict={norm_X: norm_x, norm_Y: norm_y})\n",
    "\n",
    "print(\"model: \"+str(weight[0])+\" * x1 + \"+str(weight[1])+\" * x2 + \"+str(bias[0]))    \n",
    "print(\"loss: \"+str(final_loss))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.loadtxt(\"./ex1/ex1data2.txt\", delimiter=',')\n",
    "\n",
    "X = np.array(data[:, :-1]) # (47, 2) matrix\n",
    "Y = np.array(np.reshape(data[:, -1],[-1,1])) # 47-dimension vector => (47, 1) matrix\n",
    "\n",
    "# Mean Normalization\n",
    "#norm_X = (X - np.average(X)) / (np.max(X) - np.min(X))\n",
    "#norm_Y = (Y - np.average(Y)) / (np.max(Y) - np.min(Y))\n",
    "\n",
    "norm_X = (X - X.mean(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
    "norm_Y = (Y - Y.mean(axis=0)) / (Y.max(axis=0) - Y.min(axis=0))\n",
    "\n",
    "LEARNING_RATE = 1\n",
    "NUM_STEPS = 1000\n",
    "\n",
    "n = np.shape(X)[1] # number of features = 2\n",
    "m = np.shape(X)[0] # number of training examples = 47\n",
    "\n",
    "#w = np.reshape(np.random.rand(n),[-1,1]) # 2-dimension vector => (2, 1) matrix\n",
    "w = np.array([[0],[0]]) # 2-dimension vector => (2, 1) matrix\n",
    "b = np.reshape(np.zeros(m),[-1,1]) # 47-dimension vector => (47, 1) matrix \n",
    "\n",
    "def hypothesis(w, b):\n",
    "    return np.add(np.matmul(norm_X, w), b) # (47, 2) * (2, 1) + (47, 1) = (47, 1)\n",
    "\n",
    "def computeCost(w, b):\n",
    "    loss = np.dot(1/(2*m), np.sum(np.square(np.subtract(hypothesis(w, b), norm_Y)))) # scala value\n",
    "    return loss\n",
    "\n",
    "def gradientDescent(w, b):\n",
    "    diff = np.transpose(hypothesis(w, b) - norm_Y) # transpose((47, 1) - (47, 1)) => (1, 47)\n",
    "    \n",
    "    w_gradient = LEARNING_RATE * 1/m * np.matmul(diff, norm_X) # matmul((1, 47), (47, 2)) => (1, 2)\n",
    "    b_gradient = LEARNING_RATE * 1/m * np.sum(hypothesis(w, b) - norm_Y) # sum((47, 1) - (47, 1)) => scala value\n",
    "    #b_gradient = LEARNING_RATE * 1/m * np.matmul(diff, np.ones([47, 1])) # matmul((1, 47), (47, 2)) => (1, 2)\n",
    "\n",
    "    w = w - np.transpose(w_gradient) # (2, 1) - transpose(1, 2) = (2, 1)\n",
    "    b = b - b_gradient # (47, 1) - scala value = (47, 1)\n",
    "    #b = b - np.transpose(b_gradient) # (47, 1) - scala value = (47, 1)\n",
    "    \n",
    "    return w, b\n",
    "\n",
    "def showCostFunction(input, output):\n",
    "    plt.figure(figsize=(10,7))\n",
    "    plt.plot(input, output, 'b-') \n",
    "    plt.grid(True)\n",
    "    plt.title(\"Cost Function\")\n",
    "    plt.xlabel(\"Number of Steps\")\n",
    "    plt.ylabel(\"Cost (J(w,b))\")\n",
    "\n",
    "def weightChange(step_index, step_w1, step_w2):\n",
    "    fig = plt.figure(figsize=(12,12))\n",
    "    ax = fig.gca(projection='3d')\n",
    "    \n",
    "    ax.set_title(\"Weight Change\")\n",
    "    ax.set_xlabel(\"num_steps\")\n",
    "    ax.set_ylabel(\"weight1\")\n",
    "    ax.set_zlabel(\"weight2\")\n",
    "    \n",
    "    ax.scatter(step_index, step_w1, step_w2)\n",
    "\n",
    "def dataPlot():\n",
    "    fig = plt.figure(figsize=(12,12))\n",
    "    ax = fig.gca(projection='3d')\n",
    "    \n",
    "    x1 = np.linspace(np.min(norm_X[:,0]), np.max(norm_X[:,0]), 100)\n",
    "    x2 = np.linspace(np.min(norm_X[:,1]), np.max(norm_X[:,1]), 100)   \n",
    "    norm_pred = w[0][0]*x1 + w[1][0]*x2 + b[0][0]\n",
    "\n",
    "    ax.set_title(\"Data Plot\")\n",
    "    ax.set_xlabel(\"x1\")\n",
    "    ax.set_ylabel(\"x2\")\n",
    "    ax.set_zlabel(\"y\")\n",
    "    \n",
    "    ax.scatter(norm_X[:,0], norm_X[:,1], norm_Y)   \n",
    "    ax.scatter(x1, x2, norm_pred)\n",
    "    \n",
    "def test(test_x):\n",
    "    \n",
    "    test_norm_X = (test_x - X.mean(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
    "    norm_pred = test_norm_X[0]*w[0][0] + test_norm_X[1]*w[1][0] + b[0][0]\n",
    "\n",
    "    pred = norm_pred * (np.max(Y) - np.min(Y)) + np.average(Y)\n",
    "    return pred   \n",
    "    \n",
    "step_loss = np.zeros(NUM_STEPS)\n",
    "step_index = np.zeros(NUM_STEPS)\n",
    "step_w1 = np.zeros(NUM_STEPS)\n",
    "step_w2 = np.zeros(NUM_STEPS)\n",
    "index = 0\n",
    "\n",
    "for i in range(0, NUM_STEPS):\n",
    "    loss = computeCost(w, b)\n",
    "    step_loss[i] = loss\n",
    "    step_index[i] = index\n",
    "    \n",
    "    w, b = gradientDescent(w, b)\n",
    "    step_w1[i] = w[0][0]\n",
    "    step_w2[i] = w[1][0]\n",
    "    \n",
    "    index = index + 1\n",
    "\n",
    "# prediction값을 결정할 때 x1 데이터가 x2 데이터보다 영향력이 크다\n",
    "print(\"model: \"+str(w[0][0])+\" * x1 + \"+str(w[1][0])+\" * x2 + \"+str(b[0][0]))\n",
    "print(\"loss: \"+str(loss))\n",
    "\n",
    "test_x = [4478, 5]\n",
    "pred = test(test_x)\n",
    "print(\"prediction (x1: \"+str(test_x[0])+\", x2: \"+str(test_x[1])+\"): \"+str(pred))\n",
    "\n",
    "showCostFunction(step_index, step_loss)\n",
    "weightChange(step_index, step_w1, step_w2)\n",
    "dataPlot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
